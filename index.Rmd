---
title: "Computational Musicology"
author: 'Aresh Hemayat'
date:   'April 2021'
output: 
  flexdashboard::flex_dashboard:
    storyboard: true
    orientation: columns
    vertical_layout: fill
    theme: bootstrap
    css: stotel.css
    logo: /home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/css/imgurr.gif

    
---


```{r setup, include=FALSE}
library(flexdashboard)
library(tidyverse)
library(spotifyr)
library(compmus)
library(unix)
library(plotly)
library(RColorBrewer)
library(ggthemes)
library(viridis)
library(forcats)
library(tidymodels)
library(ggdendro)
library(heatmaply)
library(kknn)
library(grid)
library(gridExtra)
library(ggplot2)
library(Rmisc)
library(GGally)
library(audio)
library(circlize)
library(rstantools)
library(ggstatsplot)
library(knitr)
library(png)

```




<!-- images -->

```{r include = FALSE}
img1_path <- "/home/stotel/Downloads/l1110789_copy_-_embed_2018-compressed.png"
img1 <- readPNG(img1_path, native = TRUE, info = TRUE)
attr(img1, "info")
```



```{r include = FALSE}
img1_path <- "/home/stotel/Downloads/l1110789_copy_-_embed_2018-compressed.png"
img1 <- readPNG(img1_path, native = TRUE, info = TRUE)
attr(img1, "info")
```

```{r include = FALSE}
img2_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/tempogram.png"
img2 <- readPNG(img2_path, native = TRUE, info = TRUE)
attr(img2, "info")
```

```{r include = FALSE}
img3_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/plots/presto.png"
img3 <- readPNG(img3_path, native = TRUE, info = TRUE)
attr(img3, "info")
```

```{r include = FALSE}
img4_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/plots/ai444.png"
img4 <- readPNG(img4_path, native = TRUE, info = TRUE)
attr(img4, "info")
```

```{r include = FALSE}
img5_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/plots/ai33.png"
img5 <- readPNG(img5_path, native = TRUE, info = TRUE)
attr(img5, "info")
```

```{r include = FALSE}
img6_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/plots/ai33.png"
img5 <- readPNG(img5_path, native = TRUE, info = TRUE)
attr(img5, "info")
```

```{r include = FALSE}
img7_path <- "/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/pics/plots/plottt.png"
img7 <- readPNG(img7_path, native = TRUE, info = TRUE)
attr(img7, "info")
```

<!-- playlists --> 

```{r, include = FALSE}
fruitvale <- get_playlist_audio_features("", "23n37IlJukKL9pUJyBbSib")
creed <- get_playlist_audio_features("", "7jA12g7812CnId8erPPhUo")
everything <- get_playlist_audio_features("", "7AT5LLcwYqISVLfLaaoQQo")
panther <- get_playlist_audio_features("", "0iPhN83fKPIjBpG8vdZfLq")
wish <- get_playlist_audio_features("", "5a9ptUGZg5aZmGqbrgpmbw")
venom <- get_playlist_audio_features("", "5Gtki6gbXEqXY8q0AYo8Ck")
creed2 <- get_playlist_audio_features("", "36L2OTBzMVmY72fLKzSaRo")
slice <- get_playlist_audio_features("", "398opLV7fDeiTRk7d6ulHv")
tenet <- get_playlist_audio_features("", "7EXL4cAtGhd0Qaumzx96rq")
```

<!-- Titles for each scores --> 

```{r, include=FALSE}
scores <-
  bind_rows(
    fruitvale %>% mutate(category = "(2013) Fruitvale Station "),
    creed %>% mutate(category = "(2015) Creed"),
    panther %>% mutate(category = "(2017) Black Panther"),
    wish %>% mutate(category = "(2017) Death Wish "),
    venom %>% mutate(category = "(2018) Venom"),
    creed2 %>% mutate(category = "(2018) Creed II"),
    tenet %>% mutate(category = "(2020) Tenet"),
    slice %>% mutate(category = "(2018) Slice"),
    everything %>% mutate(category = "(2017) Everything, Everything")
  )

```

<!-- Genres --> 

```{r, include=FALSE}
genres <-
  bind_rows(
   
      fruitvale %>% mutate(category = "Drama/Romance"),
    creed %>% mutate(category = "Sport/Drama"),
    panther %>% mutate(category = "Action"),
    wish %>% mutate(category = "Action"),
    venom %>% mutate(category = "Action"),
    creed2 %>% mutate(category = "Sport/Drama"),
    tenet %>% mutate(category = "Action"),
    everything %>% mutate(category = "Drama/Romance")

  )

```

<!-- Summary scores --> 

```{r, include=FALSE}
scores %>%
  summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness)
  )

```

<!-- Artist features --> 

```{r, include = FALSE}
ludwig <- get_artist_audio_features('Ludwig Göransson')

```

<!-- Summary Ludwig --> 

```{r, include=FALSE}
ludwigsum <- ludwig %>%
  summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness)
  )

```

```{r, include=FALSE}
ludwigsum
```

<!-- 2e componist --->

```{r, include = FALSE}

portman <- get_artist_audio_features('Rachel Portman')

```


```{r, include=FALSE}
portmansum <- portman %>%
  summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness)
  )

```

<!-- 3e componist --->

```{r, include = FALSE}

ennio <- get_artist_audio_features('Ennio Morricone')

```


```{r, include=FALSE}
enniosum <- ennio %>%
  summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness)
  )

```
<!-- 4e componist --->

```{r, include = FALSE}

zimmer <- get_artist_audio_features('Hans Zimmer')

```


```{r, include=FALSE}
zimmersum <- zimmer %>%
  summarise(
    mean_speechiness = mean(speechiness),
    mean_acousticness = mean(acousticness),
    mean_liveness = mean(liveness),
    sd_speechiness = sd(speechiness),
    sd_acousticness = sd(acousticness),
    sd_liveness = sd(liveness),
    median_speechiness = median(speechiness),
    median_acousticness = median(acousticness),
    median_liveness = median(liveness),
    mad_speechiness = mad(speechiness),
    mad_acousticness = mad(acousticness),
    mad_liveness = mad(liveness)
  )

```
<!-- bind rows componist --->

```{r, include=FALSE}
composers <-
  bind_rows(
   
      ludwigsum %>% mutate(category = "Ludwig Goransson"),
    enniosum %>% mutate(category = "Ennio Morricone"),
    portmansum %>% mutate(category = "Rachel Portman"),
    zimmersum %>% mutate(category = "zimmersum")
    

  )
```


<!-- stat Plot --> 

Preface {.storyboard}
=========================================

### statistics {data-commentary-width=550}

```{r}
# All defaults
include_graphics(img1_path)
```


***
<h2>Computational Musicology  </h2> In short, the use of statistical techniques and statistics to study musical systems, which is more or less synonymous with the computational music theory or computer science (notes, chords, rhythms, etc., and patterns thereof). Computer musicology is a type of data science by combining computer, statistics, and information. Yet computer musicology is strongly within the modern humanities because of the scientific dimension of music and reflects on the same issues as conventional humanities. Corpus studies are probably the most frequent form of computer musicology project. In a vast selection of works of music, a corpus analysis uses algorithms to interpret statistical patterns.









```{r}
ggstatsplot::ggcorrmat(
  data = scores,
  corr.method = "spearman",
  sig.level = 0.005,
  cor.vars = playlist_name:acousticness,
  cor.vars.names = c("Sepal Length", "Sepal Width", "Petal Length", "Petal Width"),
  title = "Correlalogram for Spotify's API variables",
  subtitle = "Ludwig Göransson's Scores",
  caption = expression(
    paste(
      italic("Note"),
      ": X denotes correlation non-significant at ",
      italic("p "),
      "< 0.005; adjusted alpha"
    )
  )
)

```

I will look into film composer and producer Ludwig Göransson for this report. I have taken data on nine of his new Spotify  film scores and have combined all the results in the Correlalogram above. I have to zoom in to some playlist and variables because his database is very large. I'm excited to learn about my story if I can find important relationships between various variables. As you can already see there are two negative (important) correlations in the variable acousticness. I will analyse what these developments indicate and how I can show them in various ways in the following pages.



### Significance? {data-commentary-width=550}

<b> Acousticness? </b> So what actually is acousticness? It's not that difficult, and quite self-explanatory. It means how acoustic a track is. I believe that Göransson is keen to study because he has a great combination between acoustic and electronic sounds. They are well balanced, as you can see from the Tenet ratings. His latest score is also this. Looking even at fruitvale station for example, you find that the mean acousticness is much higher.

<iframe width="560" height="315" src="http://www.youtube.com/embed/9bZkp7q19f0?rel=0" frameborder="0" allowfullscreen></iframe>
<!--
```{r}
 
stats <- genres %>% select(danceability, loudness, energy, instrumentalness, acousticness, valence,  playlist_name)

```


```{r}
data(stats)
ggpairs(stats, columns = 3:5, ggplot2::aes(colour=playlist_name)) + theme_calc(base_family = "Liberation Serif")
```

```{r include = TRUE}
  ggstatsplot::ggbetweenstats(
  data = scores,
  x = playlist_name, 
  y = acousticness,
  messages = TRUE,
  textsize = 4,
  pairwise.comparisons = TRUE,
  outlier.tagging = TRUE,
  outlier.coef = 1.5,
  outlier.label.args = list(color = "red"),
  outlier.label = track.name,
  ggtheme = ggthemes::theme_calc(),
  plot.type = "boxviolin",
  package = "ggsci",
  palette = "nrc_npg",
  type = "robust"

  
  
) + scale_x_discrete(guide = guide_axis(n.dodge = 2)) 
```
-->


***



```{r}
# All defaults
include_graphics(img7_path)
```




Analysis {.storyboard}
=========================================

### Main

```{r, include=FALSE}
lolli <-
  bind_rows(
   
      fruitvale %>% mutate(category = "Drama/Romance"),
    creed2 %>% mutate(category = "Sport/Drama"),
    tenet %>% mutate(category = "Action"),
    panther %>% mutate(category = "Action")
    

  )

```


```{r include = TRUE}
lolli <- lolli %>% mutate(track.name = fct_reorder(track.name, acousticness, last))%>%
ggplot(aes(x = `track.name`, y = `acousticness`, color = track.album.name)) + 
  geom_point(stat='identity', fill="grey", size= 3) +
  scale_size("track.popularity") +
  geom_segment(aes(y = 0.5, 
                   x = `track.name`, 
                   yend = `acousticness`, 
                   xend = `track.name`), 
                   color = "orange",
               alpha = 0.9
               ) +
  geom_text(color="white", size=2, label = "") +
  labs(title="Comparing different scores", 
       subtitle="Ludwig Göransson") + 
  ylim(0, 1) +
  coord_flip()+ scale_color_viridis(alpha = 1, begin = 0.01, end = 0.99, direction = 1,
  discrete = TRUE, option = "viridis",
) + theme_calc(base_family = "Liberation Serif")



```

```{r include = TRUE}
ggplotly(lolli)
```

***
A first step in improving the interpretation of a certain signal is to divide into building blocks more usable for the following processing steps. If these blocks consist of sinusoidal functions, a method of this kind is often referred to as the Fourier analysis. Sinusoidal functions are particular since they have an explicit physical significance with regard to frequency. The subsequent decomposition then unfolds the frequency range of a signal, like a mirror that can be used to divide light into its spectral constituent colours. The Fourier transforms a signal dependent on time into a frequency representation. As one of the main instruments in signal processing, we are going to meet a number of music processing activities with the Fourier transformation system.

Again looking at acousticness, you'll see a significant difference between Fruitvale station (Romance) and Tenet (Action). I used this chart to pick out certain tracks that will be analysed on a deeper level later on. 

### Introduction


<!-- Second Plot --> 


```{r}
scatter <-scores %>%                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = acousticness,
      y = instrumentalness,
      size = loudness,
      colour = mode
    )
  ) +
  geom_point() +              # Scatter plot.
  geom_rug(size = 0.1) +      # Add 'fringes' to show data distribution.
  geom_text(                  # Add text labels from above.
    aes(
      x = acousticness,
      y = instrumentalness,
      label = label,
      color = mode
    ),
    data = 
      tibble(
        label = c("Kilmonger", "RAINY NIGHT IN TALLINN"),
        category = c("(2017) Black Panther", "(2020) Tenet"),
        acousticness = c(0.5230, 0.1090),
        instrumentalness = c(0.932, 0.869000
)
      ),
    colour = "black",         # Override colour (not mode here).
    size = 2.5,                 # Override size (not loudness here).
    hjust = "left",           # Align left side of label with the point.
    vjust = "bottom",         # Align bottom of label with the point.
    nudge_x = -0.05,          # Nudge the label slightly left.
    nudge_y = 0.02            # Nudge the label slightly up.
  ) +
  facet_wrap(~category) +     # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Set2"        # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Acousticness",
    y = "Instrumentalness",
    colour = "Mode"
  ) +
  theme_calc() +
  
  scale_color_viridis(alpha = 1, begin = 0.61, end = 0.3, direction = 1,
  discrete = TRUE, option = "viridis",
)
  


```


```{r}
ggplotly(scatter)
```


***

As you can see Black Panther's has some really interesting findings. For example, "Kilmonger challenge" has one of the highest loudness (as seen in the point size), and "Kilmonger" has one of the highest Instrumentalness value.  Another interesting finding is the song "Rainy night in Talin" from Tenet. This song has a really high acousticness, high energy and a BPM of 130. The findings above are on the ''action/superhero" spectrum, and I want to compare these with songs from its opposite genre: "Romance". 




<!-- Third Plot --> 



### Sci-fi or action?

```{r}
scatter2 <-genres %>%                    # Start with awards.
  mutate(
    mode = ifelse(mode == 0, "Minor", "Major")
  ) %>%
  ggplot(                     # Set up the plot.
    aes(
      x = acousticness,
      y = instrumentalness,
      size = loudness,
      colour = category
    )
  ) +
  geom_point(alpha = 9/10) +              # Scatter plot.
  geom_rug(size = 0.5) +      # Add 'fringes' to show data distribution.
  geom_text(                  # Add text labels from above.
    aes(
      x = acousticness,
      y = instrumentalness,
      label = label,
      color = category
    ),
    data = 
      tibble(
        label = c("", ""),
        category = c("", ""),
        acousticness = c(0.090, 0.123),
        instrumentalness = c(0.101, 0.967)
      ),
    colour = "black",         # Override colour (not mode here).
    size = 3,                 # Override size (not loudness here).
    hjust = "left",           # Align left side of label with the point.
    vjust = "bottom",         # Align bottom of label with the point.
    nudge_x = -0.05,          # Nudge the label slightly left.
    nudge_y = 0.02            # Nudge the label slightly up.
  ) +
  facet_wrap(~mode) +     # Separate charts per playlist.
  scale_x_continuous(         # Fine-tune the x axis.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),   # Use grid-lines for quadrants only.
    minor_breaks = NULL       # Remove 'minor' grid-lines.
  ) +
  scale_y_continuous(         # Fine-tune the y axis in the same way.
    limits = c(0, 1),
    breaks = c(0, 0.50, 1),
    minor_breaks = NULL
  ) +
  scale_colour_brewer(        # Use the Color Brewer to choose a palette.
    type = "qual",            # Qualitative set.
    palette = "Set2"        # Name of the palette is 'Paired'.
  ) +
  scale_size_continuous(      # Fine-tune the sizes of each point.
    trans = "exp",            # Use an exp transformation to emphasise loud.
    guide = "none"            # Remove the legend for size.
  ) +
  theme_light() +             # Use a simpler theme.
  labs(                       # Make the titles nice.
    x = "Acousticness",
    y = "Instrumentalness",
    colour = "Genre"
  ) +
  scale_color_viridis(alpha = 1, begin = 0.9, end = 0.05, direction = 1,
  discrete = TRUE, option = "viridis"
) + theme_calc(base_family = "Liberation Serif")



```


```{r}
ggplotly(scatter2)
```






Ludde {.storyboard}
=========================================

### Ludwig Ludwig

```{r include = FALSE}
data <- read.csv("/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/composers.csv", row.names = 13)
```

```{r include = FALSE}

data <- as.matrix(data)

```

```{r include = FALSE}
chordDiagram(data)
```


```{r include = FALSE}

chordDiagram(data, annotationTrack = "grid", preAllocateTracks = 1)

circos.trackPlotRegion(track.index = 2, panel.fun = function(x, y) {
  xlim = get.cell.meta.data("xlim")
  ylim = get.cell.meta.data("ylim")
  sector.name = get.cell.meta.data("sector.index")

circos.text(mean(xlim), ylim[1] + 2.5, sector.name, 
              facing = "clockwise", niceFacing = TRUE, adj = c(0, 0.5), cex=0.6)
  

circos.axis(h = "top", labels.cex = 0.5, major.tick.percentage = 0.2, 
              sector.index = sector.name, track.index = 2)
}, bg.border = NA)
```


```{r include = FALSE}


data <- read.csv("/home/stotel/Documents/Scola/2020-2021/semester 2/blok 1/Computational Musicology/composers.csv", row.names = 13)

```

```{r include = FALSE}
data <- as.matrix(data)

```

```{r include = FALSE}
col = c(mean_speechiness="#f39f7e" , mean_acousticness="#f3a97e" , mean_liveness="#f3b27e",
             sd_speechiness="#f3bc7e" , sd_acousticness="#f3c67e" , sd_liveness="#ff9966", 
             median_speechiness="#ffb38d" , median_acousticness="#ff6517", median_liveness="#ff5803", 
             mad_speechiness="#ff1903" , mad_acousticness="#e91400" , mad_liveness="#9d0d00", 
             Ludwig_Goransson= "#7eb5f3" , Ennio_Morricone= "#f37eb5", Rachel_Portman= "#f37eb5" , category= "#7ef3bc" )

```


```{r echo=FALSE, out.width="50%"}

chordDiagram(data, grid.col = col, annotationTrack = "grid", preAllocateTracks = 1, scale = FALSE)


circos.trackPlotRegion(track.index = 2, panel.fun = function(x, y) {
  xlim = get.cell.meta.data("xlim")
  ylim = get.cell.meta.data("ylim")
  sector.name = get.cell.meta.data("sector.index")


  
circos.text(mean(xlim), ylim[1] + 2.5, sector.name, 
              facing = "clockwise", niceFacing = TRUE, adj = c(0, 0.5), cex=0.4)


circos.axis(h = "top", labels.cex = 0.5, major.tick.percentage = 0.2, 
              sector.index = sector.name, track.index = 2)
}, bg.border = NA)



```




***

chorddiagramm

<br>



<!-- Chroma Plot --> 


Chroma {.storyboard} 
=========================================


### Chroma {data-commentary-width=400}

```{r}
fma <-
  get_tidy_audio_analysis("5hETAXfUxrdMJOeL20lx4N") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

```

```{r}
rtc <-
  get_tidy_audio_analysis("5nwx668xcmm6h5msYiT3Hj") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

```

```{r}
tip <-
  get_tidy_audio_analysis("2qwjVyRjKzownq7ggOcgj8") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

```



```{r}
ch1 <- fma %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 1,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_calc(base_family = "Liberation Serif") +
  scale_fill_viridis_c() 
  #geom_vline(xintercept = 50, linetype="dotted", 
               # color = "red", size=1) + 
  #labs(title="Comparing different scores", 
     #  subtitle="Ludwig Göransson") 
  

```


```{r}
ch2 <- rtc %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 1,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_calc(base_family = "Liberation Serif") +
  scale_fill_viridis_c()
  

```

```{r}
ch3 <- tip %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 1,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_calc(base_family = "Liberation Serif") +
  scale_fill_viridis_c() 
  #geom_vline(xintercept = 50, linetype="dotted", 
               # color = "red", size=1) 
  

```

```{r}


ply1 <- ggplotly(ch1)
ply2 <- ggplotly(ch2)
ply3 <- ggplotly(ch3)

subplot(ply1, ply2, ply3, nrows=3)
```



***
From top to down:

From Mumbai to Amalfi - Acousticness: 0.9470<br>
Retrieving the case   - Acousticness: 0.4650<br>
Trucks in place       - Acousticness: 0.0328<br>

the human pitch vision is periodic in that two pitches are seen as identical in "color" as they differentiate from one octave (playing a similar harmonic role). Based on this observation, a pitch is divided into two parts known as tone height and chroma. The tone height corresponds to the number of harmonics and the chroma to the attributes found in the collection. We list chroma values in[0:11] where 0 is referred to as chroma C, 1 to C, etc. . A pitch class is the collection of all pitches with a common chroma. The music synchronization class pitch 1243 that is the chroma C class is set, comprising all pitches separated by an octave. For convenience, we interchange the expressions chroma and pitch.
By summing all the pitch factor coefficients that belong to the same chroma, a chroma representation or <b>chromagram</b> can be derived.


In the case of chroma I picked one songs with a low acousticnes (around 0), one with a high acousticness (around 1), and one with an average acousticness (around 0.5). I wanted to research if there were any interesting results in terms of Chroma features.





















### Dynamic Time Warping


```{r include = FALSE}

## wakanda original
original <-
  get_tidy_audio_analysis("2mjbf9ZvCoYqTi0RZ2FTUJ") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
## wakanda remix
remix <-
  get_tidy_audio_analysis("1u6PWhxFTHs0OuTfh5Mh79") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)


```

```{r include = TRUE}
compmus_long_distance(
  original %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  remix %>% mutate(pitches = map(pitches, compmus_normalise, "chebyshev")),
  feature = pitches,
  method = "euclidean"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Wakanda Original", y = "Dj Dahi Remix", font.size=5) +
  theme_minimal() +
  scale_fill_viridis_b(guide = NULL)

```

***


dynamic time warping(DTW)


We saw how various music representations can be compared by transforming them into appropriate function representations. Next, we are investigating how these features can be aligned or synchronized to temporarily react. We add an important technology known as dynamic time warping to this end (DTW). Two specific sequences are to be compared by DTW. These two sequences, for instance, also depict two separate variations of the same piece of music in our music synchro-nization scenario.






Saturation {.storyboard} 
=========================================

```{r include = FALSE}
rnit <-
  get_tidy_audio_analysis("5r6l0sPvfeAPjQ67fCgOwG") %>% # Change URI.
  compmus_align(tatums, segments) %>%                     # Change `bars`
  select(tatums) %>%                                      #   in all three
  unnest(tatums) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  )
```

```{r include = FALSE}
liab <-
  get_tidy_audio_analysis("7640a8QzLN6txA7Fa2q8oo") %>% # Change URI.
  compmus_align(tatums, segments) %>%                     # Change `bars`
  select(tatums) %>%                                      #   in all three
  unnest(tatums) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  )
```

```{r include = FALSE}
dt <-
  get_tidy_audio_analysis("5YO81jyIFnaqF7v0qMwuU7") %>% # Change URI.
  compmus_align(tatums, segments) %>%                     # Change `bars`
  select(tatums) %>%                                      #   in all three
  unnest(tatums) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "acentre", norm = "manhattan"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "max", norm = "chebyshev"              # Change summary & norm.
      )
  )
```


```{r include = FALSE}
cg1 <- liab %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_calc(base_family = "Liberation Serif")
```

```{r}
cg2 <- rnit %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_calc(base_family = "Liberation Serif")
```

```{r}
cg3 <- dt %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_calc(base_family = "Liberation Serif")
```
### Cepstograms



```{r include = TRUE}
a <- cg1
b <- cg2
c <- cg3

grid.newpage()
pushViewport(viewport(layout = grid.layout(2, 2)))
vplayout <- function(x, y) viewport(layout.pos.row = x, layout.pos.col = y)
print(a, vp = vplayout(1, 1))
print(c, vp = vplayout(1, 2))
print(b, vp = vplayout(2, 1:2))

```

```{r include = FALSE}
plots <- list(cg1, cg2)
```

```{r}

```


***

Analysis of Music Structure The hierarchical system in which music is arranged is one of the characteristics separating music from random sound sources. At the lowest level, there are occurrences such as individual notes, distinguished by their tone, timbre, timing and length. When different sound events are combined, large structures like motifs, sentences and sections are obtained, and these structures shape newly larger structures which define the overall design of the composition. This higher degree of structure is often referred to as the musical structure of the work, which is defined in musical sections and in relations with one another. The intro, the chorus and the verse parts of the song can be, for instance, in popular music. The exhibition, the creation and the recapitalisation of a movement may be any classical music. The general purpose of analysis of music structures is to divide a given musical representation into temporal segments which correspond to musical sections.


<b>"You know. Yeah. Time to wake up"</b>

<b>"This shit is about to go down." </b>

I will analyse the song "RAINY NIGHT IN TALIN", wich in my opinion is one of his most interesting scores. I think the build up is done perfectly and the tension is huge. Also i like the timbre of the sound, because it's hard to tell whether he used acoustic or electronic instruments. 

In the beginning, you can hear guitars and strings. It's just in two notes, but the movement and phrasing of these two notes were intriguing because it's difficult to say whether they're forward, reverse, or reversed.

After that you hear the middle part of the track, which is, when the actor sees some other army men running with gas tanks in the direction of the house.

And there's a lot of elements going on here, and you can't really tell what it is and what sound it is, but what's happening is that it's actually it's guitars again.

The almost sound like alarms. 

<i>"It's very effective. And it sounds like it's like almost like a, you know, what do you have?
Like when you when you try to wake you up in the morning like a clock,
you know. Yeah. Time to wake up.But I guess it's telling the
audience, hey, guys, time to wake. This shit is about to go down."</i>



A major contrast can be seen between this very low-acoustic (third plot) track and the track "Dinner time" (second plot') whith a very high accousticness. The album Tenet contains a variety of transitions and stories and has the perfect mix of acoustic instruments and electronic modular synthesizers. I believe the style quite well reflects the development of Ludwig. "Rainy night in Talin" sounds very enigmatic and very mysterious to me. It was also produced inside this pandemic so that perhaps it also had certain influences.

### Self-Similarty Matrices


```{r}
 rnit %>%
  compmus_self_similarity(timbre, "chebyshev") %>% 
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_fixed() +
  scale_fill_viridis_c(guide = "none") +
  theme_calc(base_family = "Liberation Serif") +
  labs(x = "", y = "")
```



***


We have shown that the concepts of repetition, uniformity, and complexity are fundamental for segmenting a given audio recording into musically significant structural components. One method of studying structural and interconnective coherence is to convert the signal into a set of features and then group the features according to their relationships. This results in a self-similarity matrix (SSM), a method which is of profound significance not just for music structure analysis but also for the analysis of several kinds of dataset.

<iframe width="560" height="315" src="https://www.youtube.com/embed/6A5EpqqDOdk" frameborder="0" allowfullscreen></iframe>


Keys {.storyboard}
=========================================

### Where are my keys? {data-commentary-width=400}

```{r include = FALSE}

circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )

```

```{r include = FALSE}
picking <-
  get_tidy_audio_analysis("5UVsbUV0Kh033cqsZ5sLQi") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```


```{r include = FALSE}
emil <-
  get_tidy_audio_analysis("4i5h0deyPfSt9qvYQSb5dZ") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```



```{r include = FALSE}
time <-
  get_tidy_audio_analysis("1Ps1H1hS80nWmx1U5jvhH4") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```



```{r include = FALSE}
challa <-
  get_tidy_audio_analysis("4i4zp0U58PhvXm6STYiL5m") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```



```{r include = FALSE}
tip <-
  get_tidy_audio_analysis("2qwjVyRjKzownq7ggOcgj8") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```



```{r include = FALSE}
fc <-
  get_tidy_audio_analysis("2JJWhWLXqXPm39suPT652C") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```








```{r}
k1 <-picking %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}
k2 <-emil %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}
k3 <-time %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}
k4 <-challa %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}
k5 <-tip %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}
k6 <-fc %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "euclidean",  # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```


```{r}


ply1 <- ggplotly(k1)
ply2 <- ggplotly(k2)
ply3 <- ggplotly(k3)
ply4 <- ggplotly(k4)
ply5 <- ggplotly(k5)
ply6 <- ggplotly(k6)

subplot(ply1, ply2, ply3, ply4, ply5, ply6, nrows=3)
```

***

From top to down:

Picking up T            accousticness: 0.9920<br>
Emi                     accousticness: 0.9910<br><br>

It's your time          accousticness: 0.5260<br>
Killmonger vs Challa    accousticness: 0.5250<br><br>

Trucks in place         accousticness: 0.0328<br>
Fast cars               accousticness: 0.0636<br><br>


Music corresponds to the continuous sound of various notes, which create a complete structure in the listener's head. The principal elements of harmony are music structures, consisting usually of three or more pitches, at least in the Western music context. The research of harmony may be considered as a review of chord building, connection and progression. The progression of chords over time corresponds directly to what is often called the harmonic material of a music piece. These developments have a harmonic significance in the composition, description and interpretation of western tonal music, like poplar, jazz and classical music. Therefore characteristics which capture harmonic information, including music structural analysis (Chapter 4) and music recovery, are commonly applied for musical processing tasks .

Typical chord detection systems consist of two main measures, as stated in the introduction. The first step consists of cutting the recorded sound to frames and transforming each frame into an adequate functional vector. Most recognition devices use chroma audio, which is related to the tonal detail in the audio signal. For each function vector to a set of pre-defined chord models in this second stage, pattern matching techniques are used. The best match decides the mark of the chord assigned to the frame. Additional improvements are used either before the matching stage (referred to as the pre filtration) or during / after the matching step of the pattern in order to optimize chord recognition results.

In the case of chord estimation I picked two songs with a low acousticnes (around 0), two with a high acousticness (around 1), and two with an average acousticness (around 1). (around 0.5). I wanted to see if there were any interesting results in terms of key features.



Presto {.storyboard}
=========================================

### Wakanda



```{r}
# All defaults
include_graphics(img3_path)
``` 




***
<b> Energy based novelty</b> <br>
We saw that playing a note on a tool always coincided with a sudden rise in signal energy. For instance, when you touch a key, pluck a line on a guitar or hit a drum with a rock. On this basis, the transformation of the signal into an energy function indicating the local power of the signal in each instance and then searching for abrupt shifts in it is an easy way for detecting notice start-ups.

<b> spectral based novelty </b>
Startup tracking for polyphonic music gets even difficult for sound events overlapping. A low impact musical activity may be masked by a high intensity occurrence. Energy variations in one instrument's sustained period can be greater than increases in the other instrument's attack phase. Therefore, it is usually difficult to identify all occurrences using pure power-based techniques in the case of several instruments playing at the same time. The characteristics of note start, however, can depend heavily on the respective instrument type. For example, for percussive tools with an impact, a rapid spike in energy may be seen, scattered over the whole frequency spectrum. In certain frequency ranges, even polyphonic mixes such noise-like broadband transients can be observed. Although the energy of a harmonic spring in the lower part of the spectrum is more localized, in the higher-frequency area fluctuations are also well detected.

For tempo I've chosen the song Kilmonger from the Black Panther's score. I think this was an interesting song to analyse because of its different layers it contains. In the beginning you can hear a flute, which represents kilmonger's African background. The flute is pitched down and escalates into chaos. The next layer are the strings and they go up in arpeggios. These escalate and grow bigger. Then suddenly the music cuts out and 808 drums start to kick. It feels dangerous and they come out of nowhere. They have a really low bass and sound like heartbeats. The last layer is the trap beat, representing him coming from Oakland. 






Clutch {.storyboard}
=========================================



### Classification {data-commentary-width=550}

 The acousticness of songs from Göransson scores are analyzed in the previous pages. This begs the question: "can a classification algorithm be built to recognize whether the acoustic variable is a strong choice?
 
 To do this I used Random Forests. Random forests are based on a basic but essential philosophy – crowd wisdom. The explanation for the random forest model working in data science is:

"A significant number of relatively uncorrelated (trees) models working as a group would outperform all of the individual parts."


Classification is a predictive modelling problem in machine learning that predicts a class mark for a given example of input data.

For the second plot i used k-Nearest Neighbour system. 
KNN is a learning algorithm which is non-parametric. Its aim is to use a database that divides data points into many groups in order to prevent the new sample point being categorized.

***


```{r}
# All defaults
include_graphics(img4_path)
```


```{r}
# All defaults
include_graphics(img5_path)
```







